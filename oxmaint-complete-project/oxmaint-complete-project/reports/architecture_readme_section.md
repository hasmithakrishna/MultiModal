## System Architecture

![System Architecture Flowchart](docs/architecture_flowchart.png)

> End-to-end pipeline from raw data ingestion through multimodal fusion to a deployed REST API.

---

### Layer 1 - Data Ingestion

All three data sources are loaded directly into VS Code from Kaggle:

- **Sensor Data** (`sensor.csv`) - ~220K rows of time-series pump telemetry with 52 sensor channels and a `machine_status` column (`NORMAL`, `RECOVERING`, `BROKEN`). This is the primary real-time signal source.
- **Images** - 8K+ grayscale casting defect photographs organised into `ok/` and `defective/` directories. Used for visual anomaly detection.
- **Mock Text** - ChatGPT-generated work orders, maintenance requests, and equipment manuals stored as JSON and `.txt` files. Simulates the kind of maintenance records found in a real CMMS (Computerised Maintenance Management System).

Each source is loaded in its own preprocessing script and outputs a **unified JSONL record** keyed by `asset_id` and `timestamp` so all three modalities can be aligned at inference.

---

### Layer 2 - Preprocessing

Each modality has a dedicated preprocessing script:

**`sensor_preprocessing.py`**
Timestamps are parsed and 52 sensor columns are identified. Infinities and NaNs are replaced with zero. Rows are grouped into 50 pumps via chunk-based `asset_id` assignment. A sliding window (size = 50, stride = 10) converts the flat CSV into ~21K time-series windows. Labels are remapped to binary (`NORMAL = 0`, `RECOVERING/BROKEN = 1`). Windows are scaled with `StandardScaler` (fit on train only) and saved as `.npy` arrays.

**`image_preprocessing.py`**
Images are discovered recursively from `ok/` and `defective/` directories. Each image is loaded in grayscale, resized to 224×224, and normalised to `[0, 1]`. A channel dimension is added for CNN compatibility. Pseudo `asset_id` and `timestamp` values are assigned deterministically so image records can join sensor/text records. Arrays are saved as `.npy` with a stratified 80/20 split.

**`text_preprocessing.py`**
Text is lowercased and stripped of special characters. Fault indicator scores are computed for five fault types using keyword matching (`bearing_failure`, `seal_leak`, `impeller_damage`, `motor_overload`, `normal`). Sentence embeddings are generated via `all-MiniLM-L6-v2` (SentenceTransformers). Work orders and maintenance requests are saved as a unified JSONL with `asset_id` + `timestamp`. Manuals are stored under `asset_id = GLOBAL` as shared knowledge.

---

### Layer 3 - Modeling

Each modality is trained with an independent model optimised for its data type:

| Modality | Model | Output |
|---|---|---|
| Sensor | **SmallLSTM** (hidden=32, 1 layer, dropout=0.2) | `p_abnormal` - probability of abnormal state |
| Image | **DefectCNN** (3-block Conv→BN→ReLU→Pool + 2 FC layers) | `p_defect` - probability of visual defect |
| Text | **TF-IDF + Logistic Regression** (10K features, bigrams, balanced weights) | `p_risk` - calibrated probability of abnormal maintenance signal |

Models are trained independently, saved as `.pth` / `.pkl` artefacts, and loaded at inference. Each produces a single calibrated probability score which feeds the fusion layer.

---

### Layer 4 - Fusion Logic

The fusion layer implements **weighted late fusion** - it never shares gradients between modalities. Instead, it combines the output probabilities from whichever modalities are present in the request:

```
failure_probability = w₁ · p_sensor + w₂ · p_image + w₃ · p_text
```

Weights are dynamically adjusted based on which modalities are present (missing modalities are excluded, not zeroed). The fused score is then used to derive:

- `predicted_fault_type` - the dominant risk category (`normal`, `abnormal_text_signal`, etc.)
- `estimated_time_to_breakdown_hours` - a heuristic TTB estimate based on failure probability thresholds
- `top_signals` - which modality contributed most to the risk score
- `llm_explanation` - a natural-language summary generated by an LLM (Claude / GPT) that explains the prediction in plain English for maintenance engineers

This design means the API works gracefully with 1, 2, or all 3 modalities - partial inputs produce partial but still meaningful predictions.

---

### Layer 5 - API Orchestration

The inference API is built with **FastAPI** and exposes a single `POST /predict` endpoint that accepts multimodal JSON input.

**Request flow:**
1. **Input Router** inspects the request payload and detects which modalities are present (sensor array, image file, text string)
2. Each present modality is routed to its corresponding model for inference
3. Outputs are passed to the **Fusion Layer**
4. The **Response Schema** assembles the structured JSON response: `failure_probability`, `predicted_fault_type`, `estimated_time_to_breakdown_hours`, `top_signals`, `modalities_used`, per-modality analysis blocks, and `llm_explanation`
5. An **Inference Timer** records end-to-end latency in milliseconds and logs it with each response

---

### Layer 6 - Deployment

| Component | Detail |
|---|---|
| **Docker** | Full application containerised with model artefacts bundled. Single `docker-compose up` starts the service. |
| **Uvicorn** | ASGI server running the FastAPI app in production mode. |
| **Model Registry** | Versioned `.pth` (PyTorch) and `.pkl` (sklearn) files stored alongside `metadata.json` for each model. |
| **Cloud / Local** | Environment-configurable - runs on AWS, GCP, or on-premises with no code changes. |
| **CI / CD** | GitHub Actions pipeline runs tests on push and triggers redeployment on merge to `main`. |
